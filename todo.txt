- see if we should add the EOF token for the ivyscript parser as well.

- go back and do the "measure" +/- math for each individual I/O being a sample for service time.

- in LDEV config data, the path type for a (the default) host group shows type iSCSI instead of Fibre Channel.

- integrate the old catscan SVP configuration file collector into ivy.  Integrate SVP config file attributes into ivy RAID_subsystem object.

- look at flex/bison parser, JSON format for edit_rollup() and for go parameters.

// maybe we have a measure_focus_rollup and a pid_focus_rollup and they default to focus_rollup?
// How hard would it be to have measure and pid each on their own focus_rollup?
        
- Write .xls output instead of .csv output so that the formula ="xxx" can be removed.

- take a look at where each slave driver thread posts into the rollups.
  Right now each slave driver thread gets the master thread global lock
  to update the rollups.  Maybe have each slave driver thread update the
  SubintervalOutput object in the workload tracker object without a lock,
  and then once each slave driver thread has done all its workloads, post
  a command complete or something, and then have the ivymaster thread
  iterate over the workload trackers and post the results into the rollups.
  This would eliminate get/release global lock for each workload thread.
  
- put in retry mechanism if upon initial startup of the ssh command to ivyslave, the initial
  read from the pipe stalls.  (kill ssh thread, harvest.  Then try again.)

- rename parameter service_time to service_time_seconds

- test using LVM and impact LUN discovery / SCSI Inquiry
- test with multipath software (HDLM / MPIO)

- validate with "top" to see if linux CPU is being reported correctly

- new metric that you can select, "pid_haircut_service time" 
- new parameter pid_haircut="5%" where those entire histogram buckets that fit within the lowest 5%
  of the service time 

- add feature to pid loop to filter out "anomalous" I/Os for the purposes of pid loop lock / control.
       - have a separate csv line / csv file way of reporting classification as "anomalous" or "normal".
       - workload thread itself would classify the I/O service time as normal or anomalous.

- add "max_above_or_below" - during a measurement the maximum consecutive subintervals 
  that the focus metric value can remain below or remain above the target_value. to the documentation.
  
- add to_string_with_decimal_places() to documentation.

- in Excel ivy csv file loader spreadsheet - make column titles frozen when you scroll down in long csv files.

- take a look at the csv code and see if it would now be easy to add a third parameter of the file name, and to hide "set csvfile", but keep "drop csvfile".


- fix so it shows the script file location 
<Error> [CreateWorkload] - [select] clause did not select any LUNs to create the workload on.

- go through all inter-thread interlocks and ensure that they have a timeout and where possible a recovery mechanism from hiccups, or 
at least make sure the ivy main executable terminates if it's hung, and taking out the subthreads as gracefully as possible. 

- fix [hosts] so that a dotted quad in double quotes is OK.- might have been fixed already - check

- Fix DF port numbers to be 0A, 1A, not 1A 2A. - update:  This is a bug in the subsystem - the ports reported by SCSI inquiry are 1A, 2A.

-  for pid, look at if it would help to mark any subinterval where IOPS was at min_IOPS
   as invalid, like if WP were out of range for measure=on.


/////////////////////////////////////////////////////////////////////////////////////// == or add an "inside_service_time".
// Need to look at redesigning the definition of "service time".
// Now we make the "I/O start" timestamp before starting the I/O.  The issue is that
// if the device driver doesn't have an available tag, the call blocks.
// So I think maybe we should instead define the I/O start time as the
// time when the start I/O call FINISHES.
///////////////////////////////////////////////////////////////////////////////////////


// Actually, maybe just add the "I/O accepted" time stamp separately,
// so we can measure how often / how long AIO interface blocks.

        // A note here to take a look at adding an option of instead of using the target IOPS from
        // last time as "IOPS last time", use instead the measured IOPS from the last subinterval.

        // The question was would this help PID loop stability or are we doomed anyway
        // because the I/O sequencer would keep getting further and further ahead.

        // Maybe we could add a filter onto the PID loop mechanism where it maintains a
        // second total_IOPS calculation which would be triggered when the I/O sequencer
        // gets, say, one second in the future, which would drift us back into "reasonable host latency".

        // We define the "host latency" of an I/O as the amount of time from the scheduled time
        // until when the I/O has been accepted by the AIO interface.
        
            // On the to-do list:
                // Change the definition of "service time" to be from when the I/O has been accepted
                // by the AIO interface, not the time from just before you tried to launch the I/O,
                // because right now we are recording time the AIO call to start an I/O blocks
                // because the device driver doesn't have enough free tags, we record this time as
                // service time even though the I/O hasn't been launched yet.)

        // Maybe we could use a mechanism to over-ride the [EditRollup] mechanism
        // and if host latency is getting too long, we adjust total_IOPS to make host latency
        // drift towards the "OK" zone.

// Another idea is to design the feature into the I/O sequencer itself, as
// for example, if a bursty workload is being generated, you may want to
// generate fewer bursts, rather than changing what a burst looks like.







- squeeze all underscores out of parameter names in a parameter value lookup table, so maxTags and max_tags are equivalent.

- have executing a statement set the current source file line number, so that error messages could print it;

- look at making "collapse" which would move all files up out of subfolders and delete the subfolders.
- or ivyscript option to not use subfolders ...

- fix bug where edit rollup total_IOPS = "45.3" fails when total_IOPS=45.3 works.

- run ivymaster as a GUI application, and show some sort of visual representation subinterval by subinteral.
  Maybe you could offer a tree on the left that shows rollups that you pick from.
  You could pick things you wanted to monitor, and arrange them on the display area.

- automated queue depth vs. response time charts for IOPS = max

- Offer data visualization - look at grafana.org.

- examine whether to remove the timestamps from the metric_value object and
  perhaps have ivy_cmddev provide data with individual I/O timestamps instead ...

- make ivy run other than as root.  	
 
- go through and everywhere we have an interlock between threads or a pipe_driver conversation, 1) instrument timing & log, 
2) where appropriate, allow a certain threshold, analogous to the BER on a disk drive that says you can have, for example,
workload threads have to pause to wait for "continue", or "stop" for up to one second once in every 1000 subintervals
when observed at ivymaster statistically aggregating across workload threads / LUNs / hosts,
3) Force a failure for every interlock / conversation and either look at doing a retry
(why does it sometimes hang only the first time you run it, and if you try a second time,
Linux is somehow primed and it works perfectly the second time?), again either look at doing
a retry, or else make sure we don't hang and that we shutdown gracefully with an error message.

Maybe the workload thread could post "I had to wait" and post the wait time in seconds.
Then when ivyslave reports the results of a subinterval, it reports the wait flag / wait time
(or timeout and die status of gracefully terminated workload thread with harvested pid)

- test [EditRollup] with IOPS increments  / multipliers

- catch a "kill" signal and shut down subtasks gently

(James) - run LSPCI and store server details to track I/O generation capability of different kinds of server components.



Consider how an ivy test run could be filed in a database, so that these tests could be located in a kind of coordinate space when you want to calibrate a model.
	-ideally you would want to know about the hardware and software versions, the configuration, etc.

Gather config report from SVP and import into ivy subsystem object. (adapt PERL collector).




New iogenerator / DynamicFeedbackController that runs some random offset patterns to discover the underlying characteristics of the subsystem.
- Find cache size automatically. Contrast with physically installed storage capacity from config data.
- Find cache segment size, slot size, and size of LRU queue in slots.
- Find size of WP by blocksize to find segment size, slot size, size of WP slot queue
- Measure seek time by seek LBA distance at sampled points across the LUN - plot zone table accordingly.
- Measure sustained sequential throughput across the drive to plot zone table.
- Measure drive cache buffer size
- Show IOPS by host HBA or PCI bus.
- For SSDs, come up with some way to characterise how their responsiveness varies according to recent history.


